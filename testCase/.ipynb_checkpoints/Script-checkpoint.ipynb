{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder in batchs\n",
    "import os, sys\n",
    "path1 = 'D:\\VGGface2\\overlap_vggface1_2/'  #指定名称文件夹所在路径\n",
    "path2 = 'D:/cnnface/female_male_test_51_addnoise/noise_0.05/'    #新建文件夹所在路径\n",
    " \n",
    "def MkDir():\n",
    "    dirs = os.listdir(path1)\n",
    "    for dir in dirs:\n",
    "        file_name = path2 + str(dir)\n",
    "        os.mkdir(file_name)\n",
    "        \n",
    "MkDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract dnn_activation\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dnnbrain.dnn import analyzer\n",
    "from dnnbrain.dnn import io as dnn_io\n",
    "\n",
    "stim = 'D:/cnnface/female_male_test_51_addnoise/Face_template.csv'\n",
    "netloader = dnn_io.NetLoader('vggface')\n",
    "imgcropsize = netloader.img_size\n",
    "transform = transforms.Compose([transforms.Resize(imgcropsize),\n",
    "                                    transforms.ToTensor()])  \n",
    "picdataset = dnn_io.PicDataset(stim, transform=transform)\n",
    "picdataloader = DataLoader(picdataset, batch_size=8, shuffle=False)\n",
    "dnn_act = analyzer.dnn_activation(picdataloader, 'vggface', 'fc3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get classification result of network.\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "from dnnbrain.dnn.models import dnn_test_model\n",
    "from dnnbrain.dnn.io import PicDataset,NetLoader,DataLoader\n",
    "from cnnface.core.vgg_identity_recons import Vgg_identity\n",
    "\n",
    "vggid = Vgg_identity()\n",
    "vggid.load_state_dict(torch.load('F:/Code/pretrained_model/vgg_male_female_CrossEntro.pth'))\n",
    "\n",
    "imgcsv_path = 'D:\\cnnface/female_male_test_51_addnoise/rcicr_gabor_noise.csv'\n",
    "transform = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                transforms.ToTensor()])\n",
    "PicSet = PicDataset(imgcsv_path,transform)\n",
    "Picloader = DataLoader(PicSet,batch_size=32)\n",
    "\n",
    "output = dnn_test_model(Picloader,vggid)\n",
    "\n",
    "#save label to txt file\n",
    "label = output[0]\n",
    "label_f = np.argwhere(label==0)\n",
    "label_m = np.argwhere(label==1)\n",
    "\n",
    "np.savetxt('D:/cnnface/female_male_test_51_addnoise/label_f.txt',label_f)\n",
    "np.savetxt('D:/cnnface/female_male_test_51_addnoise/label_m.txt',label_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种就是很害怕的感觉，害怕出来结果意料之外。\n",
    "一个科研人不应该害怕未知，不应该自己的结果。因为实验中与理论不符，意料之外的结果才有可能掀开了真理的帘子。\n",
    "开拓未知的领域，验证未知，解释未知，变成人类所掌握的技术和理论，让冰冷地客观世界变得亲切和善。\n",
    "与人相伴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d528ad3a1010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage_processing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maverage_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/cnnface/female_male_test_51_addnoise/rcicr_sinusoid_noise.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'core'"
     ]
    }
   ],
   "source": [
    "# \n",
    "import numpy as np\n",
    "from dnnbrain.dnn import io\n",
    "import os\n",
    "from PIL import Image \n",
    "from cnnface.core.Image_processing import average_img\n",
    "\n",
    "csv_file = 'D:/cnnface/female_male_test_51_addnoise/rcicr_sinusoid_noise.csv'\n",
    "index_f = np.loadtxt('D:/cnnface/female_male_test_51_addnoise/label_sinusoid_f.txt').astype('int64')\n",
    "index_m = np.loadtxt('D:/cnnface/female_male_test_51_addnoise/label_sinusoid_m.txt').astype('int64')\n",
    "\n",
    "savepath_aver_f = 'D:/cnnface/female_male_test_51_addnoise/Face_template/classification_noise/female_aver_sinusoid.jpg'\n",
    "savepath_aver_m = 'D:/cnnface/female_male_test_51_addnoise/Face_template/classification_noise/male_aver_sinusoid.jpg'\n",
    "\n",
    "picset= io.PicDataset(csv_file)\n",
    "pre_picpath = picset.picpath\n",
    "sub_picpath = picset.picname\n",
    "\n",
    "f_picpath = [os.path.join(pre_picpath,sub_picpath[i]) for i in index_f]\n",
    "m_picpath = [os.path.join(pre_picpath,sub_picpath[i]) for i in index_m]\n",
    "\n",
    "aver_img_f = average_img(f_picpath)\n",
    "aver_img_m = average_img(m_picpath)\n",
    "\n",
    "aver_img_f.save(savepath_aver_f,quality=95)\n",
    "aver_img_m.save(savepath_aver_m,quality=95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
